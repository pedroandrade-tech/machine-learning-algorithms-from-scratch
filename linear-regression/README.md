# Linear Regression - From Scratch Implementations | Regress√£o Linear - Implementa√ß√µes do Zero

> **üá∫üá∏ English version below** | **üáßüá∑ Vers√£o em portugu√™s abaixo**

---

## üá∫üá∏ English

From-scratch implementations of Linear Regression using NumPy, demonstrating both single-feature and multiple-feature regression on the Diabetes dataset.

### Overview

These notebooks implement linear regression algorithms manually, focusing on understanding the mathematical foundations of gradient descent optimization for regression problems.

### Notebooks

1. **Linear_Regression_Single_Feature_From_Scratch.ipynb**
   - Simple linear regression with one feature (BMI)
   - Visual demonstration of cost function optimization
   - Step-by-step gradient descent

2. **Multiple_Linear_Regression_From_Scratch.ipynb**
   - Multiple linear regression with 10 features
   - Demonstrates handling multiple predictors
   - Feature scaling and convergence analysis

### Dataset

**Diabetes Dataset** (from sklearn.datasets)
- **442 samples** of diabetes patients
- **10 baseline features**: age, sex, BMI, blood pressure, and 6 blood serum measurements
- **Target**: Quantitative measure of disease progression one year after baseline

**Features used**:
- Single Feature: BMI (Body Mass Index) only
- Multiple Features: All 10 baseline variables

### Implementation Details

#### ‚úÖ Implemented From Scratch

- **Cost function**: Mean Squared Error (MSE)
- **Gradient computation**: Partial derivatives calculation
- **Gradient descent**: Iterative optimization algorithm
- **Prediction function**: Linear model evaluation
- **Performance metrics**: Manual R¬≤ calculation

#### ‚ö†Ô∏è External Dependencies (Non-Algorithm)

The following sklearn utilities are used for **data loading and evaluation only**:
- `load_diabetes`: Dataset loading
- `r2_score`: R¬≤ metric for model comparison

**Note**: The core regression algorithm (cost, gradient, optimization) is 100% from scratch.

### Mathematical Foundation

#### Linear Model
```
f(x) = w¬∑x + b
```
- Single feature: `f(x) = w*x + b`
- Multiple features: `f(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b`

#### Cost Function (MSE)
```
J(w,b) = 1/(2m) * Œ£(f(x) - y)¬≤
```

#### Gradient Descent Update
```
w = w - Œ± * ‚àÇJ/‚àÇw
b = b - Œ± * ‚àÇJ/‚àÇb
```

Where:
- `‚àÇJ/‚àÇw = 1/m * Œ£(f(x) - y) * x`
- `‚àÇJ/‚àÇb = 1/m * Œ£(f(x) - y)`

### Results

#### Single Feature Linear Regression

- **Final Cost (MSE)**: 2,531.50
- **RMSE**: 50.31 points
- **R¬≤**: 0.1462 (14.62% variance explained)
- **Training**: 3,000 iterations with Œ±=0.01

**Analysis**: Single feature (BMI) explains only ~15% of disease progression, showing the limitation of using a single predictor.

#### Multiple Linear Regression

- **Final Cost (MSE)**: 1,452.19
- **R¬≤**: 0.5102 (51.02% variance explained)
- **Training**: 10,000 iterations with Œ±=0.01
- **Features**: 10 baseline variables

**Analysis**: Using all 10 features improves prediction significantly (51% vs 15%), demonstrating the value of multiple predictors in capturing complex relationships.

### Key Features

- **Clear mathematical notation** matching theory to code
- **Iterative convergence tracking** with cost history
- **Visualization** of cost function descent
- **Modular design** with reusable functions
- **Both single and multiple feature implementations** for comparison

### Usage

#### Single Feature

```python
# Load data
diabetes = load_diabetes()
X = diabetes.data[:, 2]  # BMI feature
y = diabetes.target

# Initialize parameters
w_init = 0
b_init = 0

# Train
w_final, b_final, J_history, p_history = gradient_descent(
    X, y, w_init, b_init, 
    alpha=0.01, 
    num_iters=3000, 
    compute_cost, 
    compute_gradient
)

# Predict
predictions = w_final * X + b_final
```

#### Multiple Features

```python
# Load data
diabetes = load_diabetes()
X = diabetes.data  # All 10 features
y = diabetes.target

# Initialize parameters
w_init = np.zeros(X.shape[1])
b_init = 0

# Train
w_final, b_final = gradient_descent(
    X, y, w_init, b_init,
    compute_cost,
    compute_gradient,
    alpha=0.01,
    num_iters=10000
)

# Predict
predictions = predict(X, w_final, b_final)
```

### Requirements

```bash
pip install numpy matplotlib scikit-learn
```

### Educational Purpose

These implementations prioritize **clarity and understanding** over production-ready performance. They demonstrate:
- How linear regression works mathematically
- The mechanics of gradient descent optimization
- Difference between single and multiple feature regression
- Impact of feature selection on model performance
- MSE minimization from first principles

---

## üáßüá∑ Portugu√™s

Implementa√ß√µes do zero de Regress√£o Linear usando NumPy, demonstrando regress√£o com uma √∫nica feature e m√∫ltiplas features no dataset Diabetes.

### Vis√£o Geral

Estes notebooks implementam algoritmos de regress√£o linear manualmente, focando na compreens√£o dos fundamentos matem√°ticos da otimiza√ß√£o por gradiente descendente para problemas de regress√£o.

### Notebooks

1. **Linear_Regression_Single_Feature_From_Scratch.ipynb**
   - Regress√£o linear simples com uma feature (IMC)
   - Demonstra√ß√£o visual da otimiza√ß√£o da fun√ß√£o de custo
   - Gradiente descendente passo a passo

2. **Multiple_Linear_Regression_From_Scratch.ipynb**
   - Regress√£o linear m√∫ltipla com 10 features
   - Demonstra o tratamento de m√∫ltiplos preditores
   - Escalonamento de features e an√°lise de converg√™ncia

### Dataset

**Dataset Diabetes** (de sklearn.datasets)
- **442 amostras** de pacientes com diabetes
- **10 features base**: idade, sexo, IMC, press√£o arterial e 6 medi√ß√µes de soro sangu√≠neo
- **Alvo**: Medida quantitativa da progress√£o da doen√ßa um ano ap√≥s o baseline

**Features usadas**:
- Feature √önica: IMC (√çndice de Massa Corporal) apenas
- M√∫ltiplas Features: Todas as 10 vari√°veis base

### Detalhes da Implementa√ß√£o

#### ‚úÖ Implementado do Zero

- **Fun√ß√£o de custo**: Erro Quadr√°tico M√©dio (MSE)
- **C√°lculo do gradiente**: C√°lculo de derivadas parciais
- **Gradiente descendente**: Algoritmo de otimiza√ß√£o iterativa
- **Fun√ß√£o de predi√ß√£o**: Avalia√ß√£o do modelo linear
- **M√©tricas de performance**: C√°lculo manual de R¬≤

#### ‚ö†Ô∏è Depend√™ncias Externas (N√£o-Algor√≠tmicas)

As seguintes utilidades do sklearn s√£o usadas **apenas para carregamento de dados e avalia√ß√£o**:
- `load_diabetes`: Carregamento do dataset
- `r2_score`: M√©trica R¬≤ para compara√ß√£o de modelos

**Nota**: O algoritmo central de regress√£o (custo, gradiente, otimiza√ß√£o) √© 100% do zero.

### Fundamentos Matem√°ticos

#### Modelo Linear
```
f(x) = w¬∑x + b
```
- Feature √∫nica: `f(x) = w*x + b`
- M√∫ltiplas features: `f(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b`

#### Fun√ß√£o de Custo (MSE)
```
J(w,b) = 1/(2m) * Œ£(f(x) - y)¬≤
```

#### Atualiza√ß√£o do Gradiente Descendente
```
w = w - Œ± * ‚àÇJ/‚àÇw
b = b - Œ± * ‚àÇJ/‚àÇb
```

Onde:
- `‚àÇJ/‚àÇw = 1/m * Œ£(f(x) - y) * x`
- `‚àÇJ/‚àÇb = 1/m * Œ£(f(x) - y)`

### Resultados

#### Regress√£o Linear com Feature √önica

- **Custo Final (MSE)**: 2.531,50
- **RMSE**: 50,31 pontos
- **R¬≤**: 0,1462 (14,62% de vari√¢ncia explicada)
- **Treinamento**: 3.000 itera√ß√µes com Œ±=0,01

**An√°lise**: Uma √∫nica feature (IMC) explica apenas ~15% da progress√£o da doen√ßa, mostrando a limita√ß√£o de usar um √∫nico preditor.

#### Regress√£o Linear M√∫ltipla

- **Custo Final (MSE)**: 1.452,19
- **R¬≤**: 0,5102 (51,02% de vari√¢ncia explicada)
- **Treinamento**: 10.000 itera√ß√µes com Œ±=0,01
- **Features**: 10 vari√°veis base

**An√°lise**: Usar todas as 10 features melhora significativamente a predi√ß√£o (51% vs 15%), demonstrando o valor de m√∫ltiplos preditores para capturar rela√ß√µes complexas.

### Caracter√≠sticas Principais

- **Nota√ß√£o matem√°tica clara** conectando teoria ao c√≥digo
- **Rastreamento de converg√™ncia iterativa** com hist√≥rico de custo
- **Visualiza√ß√£o** do descenso da fun√ß√£o de custo
- **Design modular** com fun√ß√µes reutiliz√°veis
- **Implementa√ß√µes com uma e m√∫ltiplas features** para compara√ß√£o

### Uso

#### Feature √önica

```python
# Carregar dados
diabetes = load_diabetes()
X = diabetes.data[:, 2]  # Feature IMC
y = diabetes.target

# Inicializar par√¢metros
w_init = 0
b_init = 0

# Treinar
w_final, b_final, J_history, p_history = gradient_descent(
    X, y, w_init, b_init, 
    alpha=0.01, 
    num_iters=3000, 
    compute_cost, 
    compute_gradient
)

# Predizer
predictions = w_final * X + b_final
```

#### M√∫ltiplas Features

```python
# Carregar dados
diabetes = load_diabetes()
X = diabetes.data  # Todas as 10 features
y = diabetes.target

# Inicializar par√¢metros
w_init = np.zeros(X.shape[1])
b_init = 0

# Treinar
w_final, b_final = gradient_descent(
    X, y, w_init, b_init,
    compute_cost,
    compute_gradient,
    alpha=0.01,
    num_iters=10000
)

# Predizer
predictions = predict(X, w_final, b_final)
```

### Requisitos

```bash
pip install numpy matplotlib scikit-learn
```

### Prop√≥sito Educacional

Estas implementa√ß√µes priorizam **clareza e compreens√£o** ao inv√©s de performance pronta para produ√ß√£o. Elas demonstram:
- Como a regress√£o linear funciona matematicamente
- A mec√¢nica da otimiza√ß√£o por gradiente descendente
- Diferen√ßa entre regress√£o com uma e m√∫ltiplas features
- Impacto da sele√ß√£o de features na performance do modelo
- Minimiza√ß√£o do MSE desde os primeiros princ√≠pios

---

## Comparison | Compara√ß√£o

| Metric / M√©trica | Single Feature / Feature √önica | Multiple Features / M√∫ltiplas Features |
|------------------|--------------------------------|----------------------------------------|
| **Features** | 1 (BMI / IMC) | 10 (all / todas) |
| **MSE** | 2,531.50 | 1,452.19 |
| **R¬≤** | 0.1462 (14.62%) | 0.5102 (51.02%) |
| **Iterations / Itera√ß√µes** | 3,000 | 10,000 |
| **Improvement / Melhoria** | Baseline | **+249% variance explained** |

**Key Insight / Insight Principal**: Multiple features capture **3.5x more variance** than single feature, showing the importance of feature engineering.

M√∫ltiplas features capturam **3,5x mais vari√¢ncia** que uma √∫nica feature, mostrando a import√¢ncia da engenharia de features.

---

## License | Licen√ßa

Educational project - Free to use and modify.

Projeto educacional - Livre para usar e modificar.
